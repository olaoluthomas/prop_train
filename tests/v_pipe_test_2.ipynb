{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce0ec0d-154e-4dc6-a149-c2158d6a7d47",
   "metadata": {},
   "source": [
    "## Testing Vertex Pipeline for Segment 108 using Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d301328-e84b-4a37-ad5d-5f21e0348daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Artifact, Output, ClassificationMetrics, Model, Metrics, Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebf416-1c76-436d-a967-bc564cd1460d",
   "metadata": {},
   "source": [
    "### Define Model Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dfb1f1-3496-4c1d-b74a-672b62805e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    # base_image=\"gcr.io/dw-analytics-d01/propimage:0.1-pipe\",\n",
    "    packages_to_install=[\"pandas==1.1.5\", \"sklearn==0.24.2\", \"xgboost==1.5.2\",\n",
    "                         \"google-cloud-storage\", \"google-cloud-bigquery\",\n",
    "                         \"google-cloud-bigquery-storage\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    dataset: Input[Artifact],\n",
    "    segment: int,\n",
    "    threshold: float,\n",
    "    # model: Input[Model],\n",
    "    gcs_artifact_uri: str,\n",
    "    metrics: Output[Metrics],\n",
    "    class_metrics: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"output\", [(\"passed\", str)]):\n",
    "    \n",
    "    from proptrainer import features\n",
    "    from proptrainer.model import preprocess\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "    from sklearn.utils import compute_class_weight\n",
    "    import joblib\n",
    "    import pickle\n",
    "    # import typing\n",
    "    \n",
    "    def threshold_check(value, threshold):\n",
    "        condition = \"false\"\n",
    "        if value > threshold:\n",
    "            condition = \"true\"\n",
    "\n",
    "        return condition\n",
    "    \n",
    "    class CprPredictor(object):\n",
    "        def __init__(self):\n",
    "            return\n",
    "\n",
    "        def load(self, gcs_artifacts_uri: str):\n",
    "            gcs_client = storage.Client()\n",
    "            with open('model.joblib', 'wb') as gcs_model, open('scaler.pkl', 'wb') as gcs_scaler:\n",
    "                gcs_client.download_blob_to_file(\n",
    "                    f\"{gcs_artifacts_uri}/model.joblib\", gcs_model)\n",
    "                gcs_client.download_blob_to_file(f\"{gcs_artifacts_uri}/scaler.pkl\",\n",
    "                                                 gcs_scaler)\n",
    "            with open('scaler.pkl', 'rb') as scal:\n",
    "                scaler = pickle.load(scal)\n",
    "\n",
    "            self._model = joblib.load(\"model.joblib\")\n",
    "            self._scaler = scaler\n",
    "\n",
    "        def predict(self, instances):\n",
    "            scaled_inputs = self._scaler.transform(instances)\n",
    "            predictions = self._model.predict(scaled_inputs)\n",
    "            return predictions\n",
    "        \n",
    "        def predict_proba(self, instances):\n",
    "            scaled_inputs = self._scaler.transform(instances)\n",
    "            probabilities = self._model.predict_proba(scaled_inputs)[:, 1]\n",
    "            return probabilities\n",
    "            \n",
    "        \n",
    "    dataset = aiplatform.TabularDataset('projects/' + \n",
    "                                        dataset.uri.split('projects/')[-1])\n",
    "    table_id = dataset._gca_resource.metadata.get(\"inputConfig\").get(\n",
    "        \"bigquerySource\").get(\"uri\").split('//')[-1]\n",
    "    \n",
    "    bqclient = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "        \n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "    (SELECT\n",
    "    * \n",
    "    FROM {table_id}\n",
    "    WHERE em_segment = {segment}\n",
    "    AND IN_HOME_DT >= DATE'2021-06-01')\n",
    "    WHERE MOD(ABS(FARM_FINGERPRINT(CASE(COUPON_BARCODE AS STRING))), 100) < 2\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_d = bqclient.query(query).result().to_dataframe(\n",
    "        bqstorage_client=bqstorageclient)\n",
    "    eval_d = preprocess(eval_d)\n",
    "    \n",
    "    columns = features.feature_lookup[str(segment)]\n",
    "    inputs = eval_d[columns]\n",
    "    target = eval_d[\"TARGET_14\"]\n",
    "        \n",
    "    predictor = CprPredictor()\n",
    "    predictor.load(gcs_artifact_uri)\n",
    "    probabilities = predictor.predict_proba(inputs)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    prior = round(target.value_counts()[1] / len(target), 5)\n",
    "    classes = np.unique(target)\n",
    "    class_weights = dict(\n",
    "        zip(\n",
    "            classes,\n",
    "            compute_class_weight(class_weight=\"balanced\",\n",
    "                                 classes=classes,\n",
    "                                 y=target)))\n",
    "    cm = confusion_matrix(target,\n",
    "                          probabilities > prior,\n",
    "                          labels=classes,\n",
    "                          sample_weight=class_weights)\n",
    "    categories = [\"0\", \"1\"]\n",
    "    class_metrics.log_confusion_matrix(categories=categories, matrix=cm.tolist())\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, probabilities)\n",
    "    class_metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    \n",
    "    test_auc = roc_auc_score(target, probabilities)\n",
    "    metrics.log_metric(\"auc\", test_auc)\n",
    "    passed = threshold_check(test_auc, threshold)\n",
    "    \n",
    "    return (passed,) # this would ideally be the precursor to model upload but CustomContainerTraining uploads the model already..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60a1b4-6d9a-44d6-afe6-bd44e8bc03ec",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd9bf27-19f8-4862-8ec8-b0b16c73e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import kfp.v2 as kfp\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"prop-pipeline\"\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    segment: int,\n",
    "    model_display_name: str,\n",
    "    args: str,\n",
    "    auc_threshold: float,\n",
    "    training_image_uri: str,\n",
    "    base_output_dir: str,\n",
    "    gcs_artifact_uri: str,\n",
    "    serving_container_image_uri: str,\n",
    "    # machine_type: str,\n",
    "    staging_bucket: str,\n",
    "):    \n",
    "    # Create Vertex Datasets\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=\"Propensity-Training-Data\",\n",
    "        bq_source=\"bq://dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_tiny_data\", # change to larger dataset once test is complete\n",
    "        project=project,\n",
    "    )\n",
    "    \n",
    "    testset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=\"Out-of-Time-Test-Data\",\n",
    "        bq_source=\"bq://dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_refresh_eval_data_w_margin\",\n",
    "        project=project,\n",
    "    )\n",
    "    \n",
    "    # testset_create_op.after(dataset_create_op)\n",
    "    \n",
    "    # Train the model with Custom Container\n",
    "    train_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"prop-model-108-gcc-training\",\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        training_fraction_split=0.79,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.01,\n",
    "        bigquery_destination=\"bq://dw-bq-data-d00\",\n",
    "        container_uri=training_image_uri,\n",
    "        model_serving_container_image_uri=serving_container_image_uri,\n",
    "        model_serving_container_predict_route=\"/predict\",\n",
    "        model_serving_container_health_route=\"/health\",\n",
    "        base_output_dir=base_output_dir,        \n",
    "        args=args,\n",
    "        project=project,\n",
    "        location=region,\n",
    "        staging_bucket=staging_bucket,\n",
    "    )\n",
    "    \n",
    "    train_op.after(dataset_create_op)\n",
    "    \n",
    "    # Evaluate the model on out-of-time data\n",
    "    eval_op = evaluate_model(\n",
    "        dataset=testset_create_op.outputs[\"dataset\"],\n",
    "        segment=segment,\n",
    "        threshold=auc_threshold,\n",
    "        gcs_artifact_uri=gcs_artifact_uri,\n",
    "    )\n",
    "    \n",
    "    eval_op.after(testset_create_op)\n",
    "    eval_op.after(train_op)\n",
    "    \n",
    "    # Check condition for model upload to Vertex\n",
    "    # with kfp.dsl.Condition(\n",
    "    #     eval_op.outputs[\"upload\"] == \"true\",\n",
    "    #     name=\"upload_model\",\n",
    "    # ):\n",
    "    #     model_upload_op = gcc_aip.ModelUploadOp(\n",
    "    #         project=project,\n",
    "    #         display_name=model_display_name,\n",
    "    #         artifact_uri=gcs_artifact_uri,\n",
    "    #         serving_container_image_uri=serving_container_image_uri,\n",
    "    #         serving_container_health_route=\"/health\",\n",
    "    #         serving_container_predict_route=\"/predict\",\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c530c60-2311-47f9-b9e9-ccd3cef7a7e9",
   "metadata": {},
   "source": [
    "### Compile and run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9db64ea-7530-4c81-abeb-9cf6c3b21a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/prop/lib/python3.6/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/134453458552/locations/us-central1/pipelineJobs/prop-pipeline-20220725192831\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/134453458552/locations/us-central1/pipelineJobs/prop-pipeline-20220725192831')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/prop-pipeline-20220725192831?project=134453458552\n",
      "PipelineJob projects/134453458552/locations/us-central1/pipelineJobs/prop-pipeline-20220725192831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/134453458552/locations/us-central1/pipelineJobs/prop-pipeline-20220725192831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/134453458552/locations/us-central1/pipelineJobs/prop-pipeline-20220725192831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = dw-analytics-d01, job_id = 543086176353910784) is failed due to the above error.; Failed to handle the job: {project_number = 134453458552, job_id = 543086176353910784}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d15bf89584e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m job.run(\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev-ana-ainb-admin@dw-analytics-d01.iam.gserviceaccount.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/envs/prop/lib/python3.6/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/prop/lib/python3.6/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     def submit(\n",
      "\u001b[0;32m/opt/conda/envs/prop/lib/python3.6/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = dw-analytics-d01, job_id = 543086176353910784) is failed due to the above error.; Failed to handle the job: {project_number = 134453458552, job_id = 543086176353910784}\"\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import PipelineJob\n",
    "from kfp.v2 import compiler\n",
    "# from kfp.v2.google.client import AIPlatformClient\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "project = \"dw-analytics-d01\"\n",
    "region = \"us-central1\"\n",
    "segment = 108\n",
    "bucket = \"gs://ai-ml-vertex-d01\"\n",
    "gcs_path = \"dm-propensity/pc\"\n",
    "auc_threshold = 0.60999\n",
    "pipeline_root_path = f\"{bucket}/{gcs_path}/{segment}/pipeline_root\"\n",
    "working_dir = f\"{pipeline_root_path}/{timestamp}\"\n",
    "model_display_name = \"prop-108-model\"\n",
    "path = \"prop-108.json\"\n",
    "training_image_uri = \"gcr.io/dw-analytics-d01/propimage:0.1-pipe\"\n",
    "serving_container_image_uri = \"gcr.io/dw-analytics-d01/propimage:0.1-predict\"\n",
    "pipeline_display_name = \"prop-108-vertex-training-pipeline\"\n",
    "gcs_artifact_uri = f\"{working_dir}/model\"\n",
    "dataset = \"dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_tiny_data\"\n",
    "staging_bucket = bucket\n",
    "\n",
    "# hyperparameters (args) for custom container training\n",
    "max_depth = 3\n",
    "min_child_weight = 3\n",
    "max_delta_step = 0.5\n",
    "reg_lambda = 0.15\n",
    "reg_alpha = 0.3\n",
    "lr = 0.3\n",
    "gamma = 0.0075\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--segment={segment}\",\n",
    "    # f\"--dataset={dataset}\",\n",
    "    f\"--max_depth={max_depth}\",\n",
    "    f\"--min_child_weight={min_child_weight}\",\n",
    "    f\"--max_delta_step={max_delta_step}\",\n",
    "    f\"--reg_lambda={reg_lambda}\",\n",
    "    f\"--reg_alpha={reg_alpha}\",\n",
    "    f\"--gamma={gamma}\",\n",
    "    f\"--lr={lr}\",\n",
    "]\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=path,\n",
    ")\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=pipeline_display_name,\n",
    "    template_path=path,\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    location=region,\n",
    "    parameter_values={\n",
    "        \"project\": project,\n",
    "        \"base_output_dir\": working_dir, \n",
    "        \"region\": region,\n",
    "        \"staging_bucket\": staging_bucket,\n",
    "        \"segment\": segment,\n",
    "        \"model_display_name\": model_display_name,\n",
    "        \"auc_threshold\": auc_threshold,\n",
    "        \"training_image_uri\": training_image_uri,\n",
    "        \"gcs_artifact_uri\": gcs_artifact_uri,\n",
    "        \"serving_container_image_uri\": serving_container_image_uri,\n",
    "        \"args\": CMDARGS,\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    service_account=\"dev-ana-ainb-admin@dw-analytics-d01.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f155e-1bad-4324-b181-ab026601488e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python [conda env:prop]",
   "language": "python",
   "name": "conda-env-prop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
