{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d301328-e84b-4a37-ad5d-5f21e0348daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Artifact, Output, ClassificationMetrics, Model, Metrics, Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebf416-1c76-436d-a967-bc564cd1460d",
   "metadata": {},
   "source": [
    "### Define Model Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dfb1f1-3496-4c1d-b74a-672b62805e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    base_image=\"gcr.io/dw-analytics-d01/propimage:0.1-pipe\",\n",
    "    # packages_to_install=[\"pandas==1.1.5\", \"sklearn==0.24.2\", \"xgboost==1.5.2\",\n",
    "    #                      \"google-cloud-storage\", \"google-cloud-bigquery\",\n",
    "    #                      \"google-cloud-bigquery-storage\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    dataset: Input[Artifact],\n",
    "    segment: int,\n",
    "    threshold: float,\n",
    "    # model: Input[Model],\n",
    "    gcs_artifact_uri: str,\n",
    "    metrics: Output[Metrics],\n",
    "    class_metrics: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"output\", [(\"passed\", str)]):\n",
    "    \n",
    "    from proptrainer import features\n",
    "    from proptrainer.model import preprocess\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "    from sklearn.utils import compute_class_weight\n",
    "    import joblib\n",
    "    import pickle\n",
    "    # import typing\n",
    "    \n",
    "    def threshold_check(value, threshold):\n",
    "        condition = \"false\"\n",
    "        if value > threshold:\n",
    "            condition = \"true\"\n",
    "\n",
    "        return condition\n",
    "    \n",
    "    class CprPredictor(object):\n",
    "        def __init__(self):\n",
    "            return\n",
    "\n",
    "        def load(self, gcs_artifacts_uri: str):\n",
    "            gcs_client = storage.Client()\n",
    "            with open('model.joblib', 'wb') as gcs_model, open('scaler.pkl', 'wb') as gcs_scaler:\n",
    "                gcs_client.download_blob_to_file(\n",
    "                    f\"{gcs_artifacts_uri}/model.joblib\", gcs_model)\n",
    "                gcs_client.download_blob_to_file(f\"{gcs_artifacts_uri}/scaler.pkl\",\n",
    "                                                 gcs_scaler)\n",
    "            with open('scaler.pkl', 'rb') as scal:\n",
    "                scaler = pickle.load(scal)\n",
    "\n",
    "            self._model = joblib.load(\"model.joblib\")\n",
    "            self._scaler = scaler\n",
    "\n",
    "        def predict(self, instances):\n",
    "            scaled_inputs = self._scaler.transform(instances)\n",
    "            predictions = self._model.predict(scaled_inputs)\n",
    "            return predictions\n",
    "        \n",
    "        def predict_proba(self, instances):\n",
    "            scaled_inputs = self._scaler.transform(instances)\n",
    "            probabilities = self._model.predict_proba(scaled_inputs)[:, 1]\n",
    "            return probabilities\n",
    "            \n",
    "        \n",
    "    dataset = aiplatform.TabularDataset('projects/' + \n",
    "                                        dataset.uri.split('projects/')[-1])\n",
    "    table_id = dataset._gca_resource.metadata.get(\"inputConfig\").get(\n",
    "        \"bigquerySource\").get(\"uri\").split('//')[-1]\n",
    "    \n",
    "    bqclient = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "        \n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "    *\n",
    "    FROM\n",
    "    (SELECT\n",
    "    * \n",
    "    FROM {table_id}\n",
    "    WHERE em_segment = {segment}\n",
    "    AND IN_HOME_DT >= DATE'2021-06-01')\n",
    "    WHERE MOD(ABS(FARM_FINGERPRINT(CASE(COUPON_BARCODE AS STRING))), 100) < 2\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_d = bqclient.query(query).result().to_dataframe(\n",
    "        bqstorage_client=bqstorageclient)\n",
    "    eval_d = preprocess(eval_d)\n",
    "    \n",
    "    columns = features.feature_lookup[str(segment)]\n",
    "    inputs = eval_d[columns]\n",
    "    target = eval_d[\"TARGET_14\"]\n",
    "        \n",
    "    predictor = CprPredictor()\n",
    "    predictor.load(gcs_artifact_uri)\n",
    "    probabilities = predictor.predict_proba(inputs)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    prior = round(target.value_counts()[1] / len(target), 5)\n",
    "    classes = np.unique(target)\n",
    "    class_weights = dict(\n",
    "        zip(\n",
    "            classes,\n",
    "            compute_class_weight(class_weight=\"balanced\",\n",
    "                                 classes=classes,\n",
    "                                 y=target)))\n",
    "    cm = confusion_matrix(target,\n",
    "                          probabilities > prior,\n",
    "                          labels=classes,\n",
    "                          sample_weight=class_weights)\n",
    "    categories = [\"0\", \"1\"]\n",
    "    class_metrics.log_confusion_matrix(categories=categories, matrix=cm.tolist())\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, probabilities)\n",
    "    class_metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    \n",
    "    test_auc = roc_auc_score(target, probabilities)\n",
    "    metrics.log_metric(\"auc\", test_auc)\n",
    "    passed = threshold_check(test_auc, threshold)\n",
    "    \n",
    "    return (passed,) # this would ideally be the precursor to model upload but CustomContainerTraining uploads the model already..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60a1b4-6d9a-44d6-afe6-bd44e8bc03ec",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd9bf27-19f8-4862-8ec8-b0b16c73e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import kfp.v2 as kfp\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"prop-pipeline\")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    # segment: int,\n",
    "    # model_display_name: str,\n",
    "    # args: str,\n",
    "    # auc_threshold: float,\n",
    "    # training_image_uri: str,\n",
    "    # base_output_dir: str,\n",
    "    # gcs_artifact_uri: str,\n",
    "    # serving_container_image_uri: str,\n",
    "    # machine_type: str,\n",
    "    # staging_bucket: str,\n",
    "):\n",
    "    # Create Vertex Datasets\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=\"Propensity-Training-Data\",\n",
    "        bq_source=\"bq://dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_propensity_data_clean\",\n",
    "        project=project,\n",
    "    )\n",
    "\n",
    "    testset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=\"Out-of-Time-Test-Data\",\n",
    "        bq_source=\"bq://dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_test\",\n",
    "        project=project,\n",
    "    )\n",
    "\n",
    "    # testset_create_op.after(dataset_create_op)\n",
    "\n",
    "#     # Train the model with Custom Container\n",
    "#     train_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "#         display_name=\"prop-model-104-gcc-training\",\n",
    "#         # dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "#         # training_fraction_split=0.79,\n",
    "#         # validation_fraction_split=0.2,\n",
    "#         # test_fraction_split=0.01,\n",
    "#         # bigquery_destination=\"dw-analytics-d01\",\n",
    "#         container_uri=training_image_uri,\n",
    "#         model_serving_container_image_uri=serving_container_image_uri,\n",
    "#         model_serving_container_predict_route=\"/predict\",\n",
    "#         model_serving_container_health_route=\"/health\",\n",
    "#         base_output_dir=base_output_dir,\n",
    "#         args=args,\n",
    "#         project=project,\n",
    "#         location=region,\n",
    "#         staging_bucket=staging_bucket,\n",
    "#     )\n",
    "\n",
    "#     train_op.after(dataset_create_op)\n",
    "\n",
    "#     # Evaluate the model on out-of-time data\n",
    "#     eval_op = evaluate_model(\n",
    "#         dataset=testset_create_op.outputs[\"dataset\"],\n",
    "#         segment=segment,\n",
    "#         threshold=auc_threshold,\n",
    "#         gcs_artifact_uri=gcs_artifact_uri,\n",
    "#     )\n",
    "\n",
    "#     eval_op.after(testset_create_op)\n",
    "#     eval_op.after(train_op)\n",
    "\n",
    "#     # Check condition for model upload to Vertex\n",
    "#     with kfp.dsl.Condition(\n",
    "#             eval_op.outputs[\"upload\"] == \"true\",\n",
    "#             name=\"upload_model\",\n",
    "#     ):\n",
    "#         model_upload_op = gcc_aip.ModelUploadOp(\n",
    "#             project=project,\n",
    "#             display_name=model_display_name,\n",
    "#             artifact_uri=gcs_artifact_uri,\n",
    "#             serving_container_image_uri=serving_container_image_uri,\n",
    "#             serving_container_health_route=\"/health\",\n",
    "#             serving_container_predict_route=\"/predict\",\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c530c60-2311-47f9-b9e9-ccd3cef7a7e9",
   "metadata": {},
   "source": [
    "### Compile and run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9db64ea-7530-4c81-abeb-9cf6c3b21a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/prop/lib/python3.6/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east4/pipelines/runs/prop-pipeline-20220727152722?project=134453458552\n",
      "PipelineJob projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/134453458552/locations/us-east4/pipelineJobs/prop-pipeline-20220727152722\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import PipelineJob\n",
    "from kfp.v2 import compiler\n",
    "# from kfp.v2.google.client import AIPlatformClient\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "project = \"dw-analytics-d01\"\n",
    "region = \"us-east4\"\n",
    "segment = 104\n",
    "bucket = \"gs://datascience-modelartifacts\"\n",
    "gcs_path = \"dm-propensity/pc\"\n",
    "auc_threshold = 0.60999\n",
    "pipeline_root_path = f\"{bucket}/{gcs_path}/{segment}/pipeline_root\"\n",
    "working_dir = f\"{pipeline_root_path}/{timestamp}\"\n",
    "model_display_name = \"prop-104-model\"\n",
    "path = \"prop-104.json\"\n",
    "training_image_uri = \"gcr.io/dw-analytics-d01/propimage:0.1-pipe\"\n",
    "serving_container_image_uri = \"gcr.io/dw-analytics-d01/propimage:0.1-predict\"\n",
    "pipeline_display_name = \"prop-104-vertex-training-pipeline\"\n",
    "gcs_artifact_uri = f\"{working_dir}/model\"\n",
    "dataset = \"dw-bq-data-d00.SANDBOX_ANALYTICS.dm_pc_tiny_data\"\n",
    "staging_bucket = bucket\n",
    "\n",
    "# hyperparameters (args) for custom container training\n",
    "max_depth = 3\n",
    "min_child_weight = 5\n",
    "max_delta_step = 0.485\n",
    "reg_lambda = 0.15\n",
    "reg_alpha = 0.325\n",
    "lr = 0.25\n",
    "gamma = 0.005\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--segment={segment}\",\n",
    "    f\"--dataset={dataset}\",\n",
    "    f\"--max_depth={max_depth}\",\n",
    "    f\"--min_child_weight={min_child_weight}\",\n",
    "    f\"--max_delta_step={max_delta_step}\",\n",
    "    f\"--reg_lambda={reg_lambda}\",\n",
    "    f\"--reg_alpha={reg_alpha}\",\n",
    "    f\"--gamma={gamma}\",\n",
    "    f\"--lr={lr}\",\n",
    "]\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=path,\n",
    ")\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=pipeline_display_name,\n",
    "    template_path=path,\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    location=region,\n",
    "    parameter_values={\n",
    "        \"project\": project,\n",
    "        # \"base_output_dir\": working_dir,\n",
    "        \"region\": region,\n",
    "        # \"staging_bucket\": staging_bucket,\n",
    "        # \"segment\": segment,\n",
    "        # \"model_display_name\": model_display_name,\n",
    "        # \"auc_threshold\": auc_threshold,\n",
    "        # \"training_image_uri\": training_image_uri,\n",
    "        # \"gcs_artifact_uri\": gcs_artifact_uri,\n",
    "        # \"serving_container_image_uri\": serving_container_image_uri,\n",
    "        # \"args\": CMDARGS,\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    service_account=\"dev-ana-ainb-admin@dw-analytics-d01.iam.gserviceaccount.com\",\n",
    "    network=\"projects/134453458552/global.networks/something/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f155e-1bad-4324-b181-ab026601488e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python [conda env:prop]",
   "language": "python",
   "name": "conda-env-prop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
